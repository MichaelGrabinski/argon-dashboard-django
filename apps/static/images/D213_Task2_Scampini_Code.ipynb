{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24205c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anthony Scampini\n",
    "# 10/27/24\n",
    "# D213: Advanced Data Analytics Task 2\n",
    "# Western Governors University"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc9f433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918e968d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data from three distinct datasets into a list of lists.\n",
    "\n",
    "filenames = ['amazon_cells_labelled.txt', 'imdb_labelled.txt', 'yelp_labelled.txt']\n",
    "lists = []\n",
    "\n",
    "for filename in filenames:\n",
    "    with open(filename) as data:\n",
    "        for line in data:\n",
    "            record = line.split('\\t')\n",
    "            lists.append(record)\n",
    "            \n",
    "print(lists[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac17981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial review indicates that there is a \\n \"newline\" denotation after each 0 or 1 in the sentiment column.\n",
    "# Those need to be removed.\n",
    "\n",
    "for record in lists:\n",
    "    record[1] = record[1].replace('\\n','')\n",
    "    \n",
    "print(lists[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d614c117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reviews are analyzed to determine the presence of any special characters.\n",
    "\n",
    "char_list = []\n",
    "for record in lists:\n",
    "    for words in record:\n",
    "        for chars in words:\n",
    "            if chars not in char_list:\n",
    "                char_list.append(chars)\n",
    "                \n",
    "print(char_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2a9a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a variety of upper-case and lower-case letters, as well as special characters and numbers.\n",
    "# All non-alphanumeric characters are removed and converted to lowercase.\n",
    "\n",
    "for record in lists:\n",
    "    record[0] = record[0].lower()\n",
    "    record[0] = re.sub('[^a-zA-Z0-9\\s]', ' ', record[0])\n",
    "\n",
    "char_list = []\n",
    "for record in lists:\n",
    "    for words in record:\n",
    "        for chars in words:\n",
    "            if chars not in char_list:\n",
    "                char_list.append(chars)\n",
    "                \n",
    "print(char_list)\n",
    "\n",
    "print(lists[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2239e98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is trailing whitespace on each review and some reviews contain double-or triple spaces.\n",
    "# The double spaces and trailing whitespace is removed.\n",
    "\n",
    "for record in lists:\n",
    "    record[0] = re.sub(' +', ' ', record[0])\n",
    "    record[0] = record[0].strip()\n",
    "    \n",
    "print(lists[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e805282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To improve the model efficiency, stopwords are removed from the reviews.\n",
    "# First, the stopwords are loaded from the NLTK library.\n",
    "\n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3a13e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we already removed special characters from the reviews, we need to remove the single quotes from the stopwords\n",
    "# as they will not match.\n",
    "\n",
    "cleaned_stopwords = []\n",
    "\n",
    "for word in stopwords.words('english'):\n",
    "    cleaned_stopwords.append(word.replace('\\'',''))\n",
    "\n",
    "print(cleaned_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aa4c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords can now be removed from the reviews.\n",
    "\n",
    "for record in lists:\n",
    "    record[0] = ' '.join([word for word in record[0].split() if word not in cleaned_stopwords])\n",
    "    \n",
    "print(lists[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b54a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any one character words.\n",
    "\n",
    "for record in lists:\n",
    "    record[0] = ' '.join([word for word in record[0].split() if len(word) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd48315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cleaned reviews are imported into a dataframe.\n",
    "\n",
    "reviews = pd.DataFrame(lists, columns=['Review','Sentiment'])\n",
    "print(reviews.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4964685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dataframe for null values\n",
    "\n",
    "for (review, sentiment) in reviews.items():\n",
    "    print('Total missing values in variable %s is: ' % review + str(reviews[review].isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e436e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dataframe to ensure all sentiment values are 0 or 1.\n",
    "\n",
    "print(reviews.Sentiment.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833da3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform tokenization, acquire the vocabulary size needed for the model.\n",
    "# One is added to the vocabularly to account for a padding value.\n",
    "\n",
    "tokenized = Tokenizer()\n",
    "tokenized.fit_on_texts(reviews.Review)\n",
    "print('Vocabulary size: ', len(tokenized.word_index)+1)\n",
    "for key, val in tokenized.word_index.items():\n",
    "    print(val, ':', key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddda265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find min, median, and max for lengths of number of words in each reviews. (Elleh, n.d.)\n",
    "\n",
    "review_len = []\n",
    "for word_len in reviews.Review:\n",
    "    review_len.append(len(word_len.split(\" \")))\n",
    "    \n",
    "print('Maximum length of sequences: ', np.max(review_len))\n",
    "print('Minimum length of sequences: ', np.min(review_len))\n",
    "print('Median length of sequences: ', round(np.median(review_len)))\n",
    "print('Mean length of sequences: ', round(np.mean(review_len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb62065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum length is 44 and that value will be used for the padding.\n",
    "# In order to find the optimal word embedding length, we take the vocabulary size and acquire the fourth root.\n",
    "\n",
    "vocab_size = len(tokenized.word_index)+1\n",
    "embed_size = vocab_size**0.25\n",
    "print('Embedding size: ', embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa845c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have acquired the vocabulary size, maximum word length and optimal embedding size, the vocabulary can be\n",
    "# tokenized so that the words are represented by numbers.\n",
    "\n",
    "vocab_tokenized = Tokenizer(num_words=5019, oov_token='OOV')\n",
    "vocab_tokenized.fit_on_texts(reviews.Review)\n",
    "encoded_reviews = vocab_tokenized.texts_to_sequences(reviews.Review)\n",
    "\n",
    "print('Before: ',lists[1][0])\n",
    "print('After: ',encoded_reviews[1])\n",
    "print('')\n",
    "print('Before: ',lists[20][0])\n",
    "print('After: ',encoded_reviews[20])\n",
    "print('')\n",
    "print('Before: ',lists[50][0])\n",
    "print('After: ',encoded_reviews[50])\n",
    "print('')\n",
    "print('Before: ',lists[100][0])\n",
    "print('After: ',encoded_reviews[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae0efdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectorized reviews are then padded so that each record has an equal shape.\n",
    "# We previously determined that the maximum sequence length is 44, thus all records\n",
    "# are padded to 44.\n",
    "\n",
    "padded_reviews = pad_sequences(encoded_reviews, maxlen=44)\n",
    "padded_reviews = pd.DataFrame(padded_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2083f52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the padded reviews\n",
    "\n",
    "padded_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b44671b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reviews are split into training and testing datasets.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_reviews, reviews.Sentiment.astype(int), test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0836b169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the training and testing data to CSV for the assessment.\n",
    "\n",
    "X_train.to_csv('d213_task2_Scampini_Xtrain.csv', index=False)\n",
    "X_test.to_csv('d213_task2_Scampini_Xtest.csv', index=False)\n",
    "y_train.to_csv('d213_task2_Scampini_ytrain.csv', index=False)\n",
    "y_test.to_csv('d213_task2_Scampini_ytest.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264dac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequential model using TensorFlow with five layers:\n",
    "# Embedding, Pooling, Dense Relu, Dense Softmax, and Dense Sigmoid.\n",
    "# 24 nodes for Relu and Softmax were found to be optimal in expirimentation.\n",
    "# (Elleh, n.d.)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, round(embed_size)),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(24, activation='softmax'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=48,\n",
    "                    callbacks=tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2, restore_best_weights=True),\n",
    "                    verbose=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1582cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a line graph to visualization the accuracy and loss throughout the epochs.\n",
    "\n",
    "plt.figure(figsize = [10,4])\n",
    "plt.plot(history.history['accuracy'],label='Accuracy')\n",
    "plt.plot(history.history['loss'],label='Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b4a592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the accuracy of the model. (Elleh, n.d.)\n",
    "\n",
    "result = model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe00c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "\n",
    "model.save(\"D213_Task2_Scampini_Model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6c7b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
